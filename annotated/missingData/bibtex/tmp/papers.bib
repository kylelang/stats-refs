%%% Title:    Missing Data Journal Articles
%%% Author:   Kyle M. Lang
%%% Created:  2019-09-13
%%% Modified: 2019-10-29

@article{allison:1987,
  title = {Estimation of linear models with incomplete data},
  author = {Allison, Paul D.},
  year = {1987},
  journal = {Sociological Methodology},
  number = {1},
  pages = {71--103},
  volume = {17}
}

@article{anderson:1957,
  title = {Maximum Likelihood Estimates for a Multivariate Normal Distribution when some Observations are Missing},
  author = {Anderson, T. W.},
  year = {1957},
  annote = {
This article provides a concise mathematical workout of Maximum Likelihood estimation of multivariate
normal models with a monotone missing data pattern (from bivariate to any-variate generalisation).
},
  journal = {Journal of the American Statistical Association},
  number = {278},
  pages = {200--203},
  volume = {52}
}

@article{andridgeLittle:2010,
  title = {A review of hot deck imputation for survey non-response},
  author = {Andridge, Rebecca R and Little, Roderick J A},
  year = {2010},
  annote = {
Andrige and Little point out two main advantages of hot deck imputation methods: it results in a 
rectangular data and it does not rely on any model specification. Different measures and methods
for creating the donor pool are reviewed along with how to account for missing data patterns 
(monotone or \textit{Swiss cheese}) and how to incorporate sampling weights.
},
  journal = {International Statistical Review},
  number = {1},
  pages = {40--64},
  publisher = {Wiley Online Library},
  volume = {78}
}

@article{aydilekArslan:2013,
  title = {A hybrid method for imputation of missing values using optimized fuzzy \emph{c}-means with support vector regression and a genetic algorithm},
  author = {Aydilek, Ibrahim Berkan and Arslan, Ahmet},
  year = {2013},
  journal = {Information Sciences},
  pages = {25--35},
  volume = {233}
}

@article{baraldiEnders:2010,
  title = {An introduction to modern missing data analyses},
  author = {Baraldi, Amanda N and Enders, Craig K},
  year = {2010},
  journal = {Journal of School Psychology},
  number = {1},
  pages = {5--37},
  publisher = {Elsevier},
  volume = {48}
}

@article{borgoniBerrington:2013,
  title = {Evaluating a sequential tree-based procedure for multivariate imputation of complex missing data structures},
  author = {Borgoni, Riccardo and Berrington, Ann},
  year = {2013},
  annote = {
The authors integrate a sequential (one-variable-at-the-time + data augmentation) tree-based imputation
algorithm and non-parametric bootstrap. Specifically, given a set of fully observed inputs and 
a set of target variables with missing values, their approach grows a tree on a complete subset 
of the instances for each target variable sequentially (starting with the target variable with 
fewest missing cases, uses the imputed values to augment the original data, and improve imputation 
of variables with more missing values), and iterates this procedure until some criteria is reached. 
To account for the added uncertainty due to imputation, the authors apply this method to a large
number of bootstrap samples from the original dataset. \\

Considering only categorical variables, the authors compare mode, sequential regressions, a non-iterative
tree-based imputations to their bootstrap tree-based imputation algorithm. Most notably, the results 
of the simulation study point out that non-iterative regression trees are worst than simple
mode imputation in terms of estimation accuracy (bias and efficiency of a true model parameters
estimates), and that their bootstrap version of an imputing decision tree compensates well for the
extra variability added by the imputation process.
},
  doi = {10.1007/s11135-011-9638-3},
  journal = {Quality \& Quantity},
  number = {4},
  pages = {1991--2008},
  volume = {47}
}

@article{brasMenezes:2007,
  title = {Improving cluster-based missing value estimation of DNA microarray data},
  author = {Br\'{a}s, L\'{i}gia P and Menezes, Jos\'{e} C},
  year = {2007},
  journal = {Biomolecular engineering},
  number = {2},
  pages = {273--282},
  volume = {24}
}

@article{burgetteReiter:2010,
  title = {Multiple Imputation for Missing Data via Sequential Regression Trees},
  author = {Burgette, Lane F. and Reiter, Jerome P.},
  year = {2010},
  annote = {
The authors propose a multiple imputation via sequential decision trees to deal with large datasets with
missing data on many predictors. The approach is rooted in the MICE algorithm. The main point
of departure is the way predictive distribution draws are done. For each target variable with missing values
a tree is grown on the remaining complete (augmented) dataset. The imputations are done by locating
each case with a missing value on the current target variable in a leaf of such a tree and attributing 
a randomly sampled target variable value among the observed ones in that leaf. \\

In the simulation setup, the approach proposed works fine in terms of providing low bias of the parameter estimates
of the data generating model fitted to the imputed data. However, the coverage rates are alarmingly low.
},
  doi = {10.1093/aje/kwq260},
  journal = {American Journal of Epidemiology},
  number = {9},
  pages = {1070--1076},
  volume = {172}
}

@article{collinsEtAl:2001,
  title = {A comparison of inclusive and restrictive strategies in modern missing data procedures},
  author = {Collins, L. M. and Schafer, J. L. and Kam, C.},
  year = {2001},
  annote = {
Collins \textit{et al} investigate the effects of an inclusive \textit{vs} a restrictive strategy to the use of
auxiliary variables in the missing data handling model of either a ML (maximum likelihood) 
or MI (Multiple Imputation) approach. \\

The authors distinguish between three categories of auxiliary variables based on their 
correlation with the variable(s) of interest that presents missing values, and the missingness 
mechanism). Through 4 different simulation setups, the authors study the
consequences of including or excluding auxiliary variables, from each of these categories, 
and ultimately provide compelling evidence to prefer an inclusive strategy. \\

An interesting point is raised regarding the ease of implementation of the inclusive strategy. In particular, 
the authors point out that, while the inclusion of auxiliary variables is straightforward in the MI 
framework, current (up to 2001) software implementations of ML methods do no facilitate it. This topic 
is thoroughly explored by Graham (2003). \\

A final point stressed by the article is that the consequences of the inclusive or restrictive strategies 
heavily depend on the type of MAR. For example, in their simulations, Collins \textit{et al} found 
that the estimated mean of a variable Y of interest was biased when the auxiliary variable Z was 
excluded from the data handling model, if the probability of missingess was linearly related to Z 
(MAR-linear); however, that was not the case when the probability of missigness was larger for 
extreme values of Z or a function of the correlation between Z and Y.
},
  doi = {10.1037//1082-989X.6.4.330},
  journal = {Pscyhological Methods},
  number = {4},
  pages = {330--351},
  volume = {6}
}

@article{conversanoSiciliano:2009,
  title = {Incremental Tree-Based Missing Data Imputation with Lexicographic Ordering},
  author = {Conversano, Claudio and Siciliano, Roberta},
  year = {2009},
  annote = {
The authors introduce the Incremental NonParametric Imputation (INPI) algorithm for imputing large datasets 
with missing values on multiple categorical and/or continuous variables. This approach reorganises the 
data according to a lexicographic order (arranging the columns and rows of a data matrix so as to concentrate
the missing values in a corner), grows a decision tree with a FAST algorithm (selecting first the best
splitting predictor and then the best splitting cut off value), imputes a missing value that minimises
a given error/decision rule, and augments the complete data before repeating the process for all the 
other missing values. \\

The INPI algorithm exploits all of the advantages of decision tree methods (i.e. nonparametric nature, flexibility
to variable types) and grants an effective imputation method for large datasets. Another advantage of the 
is that the algorithm can be easily extended to include imputation model uncertainty (Multiple 
Imputation). However, it must be noticed that this algorithm assumes a MAR mechanisms and that there 
is at least one completely observed variable in the original dataset.
},
  doi = {10.1007/s00357-009-9038-8},
  journal = {Journal of Classification},
  number = {3},
  pages = {361--379},
  volume = {26}
}

@article{d'ambrosioEtAl:2012,
  title = {Accurate Tree-based Missing Data Imputation and Data Fusion within the Statistical Learning Paradigm},
  author = {D'Ambrosio, Antonio and Aria, Massimo and Siciliano, Roberta},
  year = {2012},
  annote = {
The imputation algorithm proposed in this article improves on the INPI algorithm proposed by Conversano
and Siciliano (2009) in two ways: by incrementally imputing one variable at the time, it reduces 
the computational effort compared to the imputation of single missing data points one at the time;
by using the ensemble learning classifier AdaBoost, instead of single decision tree, the new 
algorithm is more accurate, in terms of prediction error. \\

This article also integrates this imputation approach in a ``data fusion'' problem. This could be 
interesting for imputation problems with datasets gathered according to planned missing data 
designs. \\

The article interestingly includes the MICE approach as one of the imputation methods compared. However, 
it does not provide a valid measure of comparison between MICE and BINPI. The authors stress how
the performance of a BINPI algorithm should be evaluated based on its ability to recover the 
missing values (according to the Statistical Learning Theory framework). However, MICE is an imputation
strategy grounded in the stochastic framework proposed by Rubin. Hence, MICE should be 
evaluated not in terms of the ability to recover missing values (prediction error), but in terms of
how statistically valid the analysis that it allows to perform are.
},
  doi = {10.1007/s00357-012-9108-1},
  journal = {Journal of Classification},
  number = {2},
  pages = {227--258},
  volume = {29}
}

@inproceedings{deAndradeSilvaHruschka:2009,
  title = {EACImpute: an evolutionary algorithm for clustering-based imputation},
  author = {de Andrade Silva, Jonathan and Hruschka, Eduardo R},
  year = {2009},
  booktitle = {2009 Ninth International Conference on Intelligent Systems Design and Applications},
  organization = {IEEE},
  pages = {1400--1406}
}

@article{deAndradeSilvaHruschka:2013,
  title = {An experimental study on the use of nearest neighbor-based imputation algorithms for classification tasks},
  author = {de Andrade Silva, Jonathan and Hruschka, Eduardo Raul},
  year = {2013},
  annote = {
The article compares 5 popular Nearest-Neighbour algorithms based on both their prediction accuracy and 
the classification bias. The results show that in a MCAR context, the Iterative KNNImpute algorithm proposed
by Br√†s and Menezes (2007) outperforms all other methods, according to both the Normalised Root Mean Squared 
Error (NRMSE) and the Average Correct Classification Rate (ACCR) criterion. However, in a MAR scenario,
the KNNI proposed by Troyanskaya et al 2001, the Sequential KNNI proposed by Kim et al (2004), and the 
EACI proposed by de Andrade Silva and Hruschka (2009) perform best. \\

One of the clearest contribution of the article is showing that
better prediction accuracy does not necessarily imply a better modelling performance (i.e. an algorithm
might ``recover'' the missing values better, but another algorithm might provide a better estimation
of the relationship between attributes). This result motivates a departure from the exclusive use of
the Normalised Root Mean Squared Error measure of prediction accuracy as the criteria to evaluate an imputation
algorithm. It must be noted that this article is concerned exclusively with a classification task.
},
  doi = {10.1016/j.datak.2012.12.006},
  journal = {Data \& Knowledge Engineering},
  pages = {47--58},
  volume = {84}
}

@article{dempsterEtAl:1977,
  title = {Maximum likelihood from incomplete data via the EM algorithm},
  author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
  year = {1977},
  annote = {
The EM algorithm is introduced in this paper as an approach to iteratively 
compute maximum likelihood estimates when data is incomplete. \\

The algorithm is introduced first through a numerical example involving discrete variables,
then, formally, for the case in which the complete data distribution belongs to the 
exponential family. Finally, it is extended for any distributional family.

The article extensively discusses the general properties of the EM algorithm with great
attention to the technical details. In section 3, the authors provide the mathematical
justification for the effectiveness of the algorithm in finding the maximum likelihood
estimate of a vector parameter when the complete-data likelihood is unkwon/intractable.

Finally, the application of the EM algorithm is discursively exemplified in different scenarios
such as missing data, and mixture modelling.
},
  doi = {10.2307/2984875},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  pages = {1--38}
}

@article{dengEtAl:2016,
  title = {Multiple imputation for general missing data patterns in the presence of high-dimensional data},
  author = {Deng, Yi and Chang, Changgee and Ido, Moges Seyoum and Long, Qi},
  year = {2016},
  annote = {
The authors carried out a comparison study similar to that of Zhao and Long (2013) in order to assess the efficiency
of regularised regression for multiple imputation of high-dimensional data. One key contribution is 
generalising the univariate missing case to multivariate missing patterns. However, the study considers 
at most three variables afflicted by missing values (at least in the simulation study). \\

The study shows a clear dominance of MICE-IURR (MICE with indirect use of regularised regression) in terms of bias
compared to all other viable methods. In terms of coverage rate, there is no clear winner. Notably, the Random
Forest MICE approach proposed by Shah et al 2014 does not live up to the promising findings of this previous
study (the unsatisfactory performance of MICE-RF is corroborated by Drechsler and Reiter, 2011). Finally,
the result shown for MICE-DURR are also in disagreement with what was found by Zhao and Long (2013).
},
  doi = {10.1038/srep21689},
  journal = {Scientific reports},
  pages = {21689},
  publisher = {Nature Publishing Group},
  volume = {6}
}

@article{drechslerReiter:2011,
  title = {An empirical evaluation of easily implemented, nonparametric methods for generating synthetic datasets},
  author = {Drechsler, J\"{o}rg and Reiter, Jerome P.},
  year = {2011},
  annote = {
The authors compare four Machine Learning methods to create synthetic datasets: (sequential/repeated) CART, 
implemented in a similar fashion to Burgette and Reiter (2010); CART applied to bootstrapped subsamples 
of the original dataset (random forests and bagging); and Support Vector Machines. These prediction 
methods are expected to generate synthetic datasets that preserve the original complex data structure, 
and hence grant statistical validity of any secondary analyses carried out on them. \\

Generating a synthetic dataset can be thought of as imputing a dataset with missing data. The issue of granting 
statistical validity of the analyses performed on a partially synthetic dataset is the same as that 
of obtaining statistically valid analyses on an imputed dataset. Therefore, these findings are directly
applicable to the high-dimensional missing data handling problem. The study shows sequential/MI CART and 
SVM synthetic dataset generation (imputation) procedures outperform both random forests and bagging 
approaches.
},
  doi = {10.1016/j.csda.2011.06.006},
  journal = {Computational Statistics and Data Analysis},
  number = {12},
  pages = {3232--3243},
  volume = {55}
}

@article{enders:2001-mlr,
  title = {The performance of the full information maximum likelihood estimator in multiple regression models with missing data},
  author = {Enders, Craig K},
  year = {2001},
  journal = {Educational and Psychological Measurement},
  number = {5},
  pages = {713--740},
  publisher = {Sage Publications Sage CA: Thousand Oaks, CA},
  volume = {61}
}

@article{enders:2001-primer,
  title = {A primer on maximum likelihood algorithms available for use with missing data},
  author = {Enders, Craig K},
  year = {2001},
  journal = {Structural Equation Modeling},
  number = {1},
  pages = {128--141},
  publisher = {Taylor \& Francis},
  volume = {8}
}

@article{enders:2013,
  title = {Dealing with missing data in developmental research},
  author = {Enders, Craig K},
  year = {2013},
  journal = {Child Development Perspectives},
  number = {1},
  pages = {27--31},
  publisher = {Wiley Online Library},
  volume = {7}
}

@article{endersBandalos:2001,
  title = {The relative performance of full information maximum likelihood estimation for missing data in structural equation models},
  author = {Enders, Craig K and Bandalos, Deborah L},
  year = {2001},
  journal = {Structural Equation Modeling},
  number = {3},
  pages = {430--457},
  publisher = {Taylor \& Francis},
  volume = {8}
}

@article{fessantMidenet:2002,
  title = {Self-Organising Map for Data Imputation and Correction in Surveys},
  author = {Fessant, Fran\c{c}oise and Midenet, Sophie},
  year = {2002},
  doi = {10.1007/s005210200002},
  journal = {Neural Computing and Applications},
  number = {4},
  pages = {300--310},
  volume = {10}
}

@article{gabrys:2002,
  title = {Neuro-fuzzy approach to processing inputs with missing values in pattern recognition problems},
  author = {Gabrys, Bogdan},
  year = {2002},
  journal = {International Journal of Approximate Reasoning},
  number = {3},
  pages = {149--179},
  volume = {30}
}

@article{garcia-laencinaEtAl:2010,
  title = {Pattern classification with missing data: {A} review},
  author = {Garc\'{i}a-Laencina, Pedro J. and Sancho-G\'{o}mez, Jos\'{e}-Luis and Figueiras-Vidal, An\'{i}bal R.},
  year = {2010},
  doi = {10.1007/s00521-009-0295-6},
  journal = {Neural Computing \& Applications},
  number = {2},
  pages = {263--282},
  volume = {19}
}

@article{garcia-laencinaEtAl:2013,
  title = {Classifying patterns with missing values using Multi-Task Learning perceptrons},
  author = {Garc\'{i}a-Laencina, Pedro J. and Sancho-G\'{o}mez, Jos\'{e}-Luis and Figueiras-Vidal, An\'{i}bal R.},
  year = {2013},
  doi = {10.1016/j.eswa.2012.08.057},
  journal = {Expert Systems with Applications},
  number = {4},
  pages = {1333--1341},
  volume = {40}
}

@article{garcia-laencinaEtAl:2009,
  title = {\emph{K} nearest neighbours with mutual information for simultaneous classification and missing data imputation},
  author = {Garc\'{i}a-Laencina, Pedro J. and Sancho-G\'{o}mez, Jos\'{e}-Luis and Figueiras-Vidal, An\'{i}bal R. and Verleysen, Michel},
  year = {2009},
  annote = {
The authors introduce an improved version of the KNNImpute algorithm proposed by Troyanskaya 
\textit{et al.} 2001. Their proposed strategy is called MI-KNNImpute, a somewhat confusing
label: the ``MI'' portion does not refer to ``multiple imputation'' but to the concept of
Mutual Information (i.e. the reduction of the uncertainty of a variable when another one is
known). \\

The authors show with different incomplete datasets that the performance of the classification task, performed 
by a KNN algorithm that uses a distance measure of Euclidian form and one that uses MI, is 
improved by first imputing the datasets with the MI-KNNImpute algorithm compared to the standard 
KNNImpute. \\

The improved performance is achieved by including some information regarding the classification task
in the imputation phase, through the use of MI as a measure of distance between the target 
class variable and the other input attributes. The use of MI in the algorithm effectively 
weights the importance of each attribute for the imputation of the target class variable.
},
  doi = {10.1016/j.neucom.2008.11.026},
  journal = {Neurocomputing},
  number = {7--9},
  pages = {1483--1493},
  volume = {72}
}

@article{gheyasSmith:2010,
  title = {A neural network-based framework for the reconstruction of incomplete data sets},
  author = {Gheyas, Iffat A. and Smith, Leslie S.},
  year = {2010},
  doi = {10.1016/j.neucom.2010.06.021},
  journal = {Neurocomputing},
  number = {16--18},
  pages = {3039--3065},
  volume = {73}
}

@article{graham:2003,
  title = {Adding Missing-Data-Relevant Variables to FIML-Based Structural Equation Models},
  author = {Graham, John W.},
  year = {2003},
  annote = {
The author effectively shows, through multiple simulation studies, that including auxiliary 
variables, when using a FIML (Full Information Maximum Likelihood) approach to 
handling and analysing datasets with missing data, can be done relatively easily
under the structural equation modelling framework. \\

With this paper, Graham introduces the \textit{Saturated Correlated model} and 
\textit{the extra dv model} to include auxiliary variables in a SEM model. Ultimately,
he showed that it is possible to include auxiliary variables in a ML missing data 
handling procedure, without affecting the substantive model (that is to say obtaining 
the same parameter estimates, standard errors, and estimates of quality of fit of a 
substantive model that does not include them).
},
  doi = {10.1207/S15328007SEM1001_4},
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  number = {1},
  pages = {80--100},
  volume = {10}
}

@article{graham:2009,
  title = {Missing data analysis: Making it work in the real world},
  author = {Graham, John W.},
  year = {2009},
  journal = {Annual review of psychology},
  pages = {549--576},
  volume = {60}
}

@article{guptaLam:1996,
  title = {Estimating Missing Values Using Neural Networks},
  author = {Gupta, Amit and Lam, Monica S.},
  year = {1996},
  doi = {10.2307/2584344},
  journal = {The Journal of the Operational Research Society},
  number = {2},
  pages = {229--238},
  volume = {47}
}

@article{heBelin:2014,
  title = {Multiple imputation for high-dimensional mixed incomplete continuous and binary data},
  author = {He, Ren and Belin, Thomas},
  year = {2014},
  doi = {10.1002/sim.6107},
  journal = {Statistics in Medicine},
  number = {13},
  pages = {2251--2262},
  publisher = {Wiley Online Library},
  volume = {33}
}

@article{honakerKing:2010,
  title = {What to Do about Missing Values in Time-Series Cross-Section Data},
  author = {Honaker, J. and King, G.},
  year = {2010},
  doi = {10.1111/j.1540-5907.2010.00447.x},
  journal = {American Journal of Political Science},
  number = {2},
  pages = {561--581},
  publisher = {Blackwell Publishing Inc.},
  volume = {54}
}

@article{howardEtAl:2015,
  title = {Using principal components as auxiliary variables in missing data estimation},
  author = {Howard, Waylon J and Rhemtulla, Mijke and Little, Todd D},
  year = {2015},
  doi = {10.1080/00273171.2014.999267},
  journal = {Multivariate Behavioral Research},
  number = {3},
  pages = {285--299},
  publisher = {Taylor \& Francis},
  volume = {50}
}

@article{iacusPorro:2007,
  title = {Missing data imputation, matching and other applications of random recursive partitioning},
  author = {Iacus, Stefano M. and Porro, Giuseppe},
  year = {2007},
  doi = {10.1016/j.csda.2006.12.036},
  journal = {Computational Statistics \& Data Analysis},
  number = {2},
  pages = {773--789},
  volume = {52}
}

@article{jerezEtAl:2010,
  title = {Missing data imputation using statistical and machine learning methods in a real breast cancer problem},
  author = {Jerez, Jos\'{e} M. and Molina, Ignacio and Garc\'{i}a-Laencina, Pedro J. and Alba, Emilio and Ribelles, Nuria and Mart\'{i}n, Miguel and Franco, Leonardo},
  year = {2010},
  doi = {10.1016/j.artmed.2010.05.002},
  journal = {Artificial Intelligence in Medicine},
  number = {2},
  pages = {105--115},
  volume = {50}
}

@article{schaferGraham:2002,
  title = {Missing Data: Our View of State of the Art},
  author = {Joseph L. Schafer and John W. Graham},
  year = {2002},
  doi = {10.1037//1082-989X.7.2.147},
  journal = {Psychological Methods},
  number = {2},
  pages = {147--177},
  volume = {7}
}

@article{junninenEtAl:2004,
  title = {Methods for imputation of missing values in air quality data sets},
  author = {Junninen, Heikki and Niska, Harri and Tuppurainen, Kari and Ruuskanen, Juhani and Kolehmainen, Mikko},
  year = {2004},
  doi = {10.1016/j.atmosenv.2004.02.026},
  journal = {Atmospheric Environment},
  number = {18},
  pages = {2895--2907},
  volume = {38}
}

@article{kangYusof:2012,
  title = {Application of Self-Organizing Map {(SOM)} in Missing Daily Rainfall Data in Malaysia},
  author = {Kang, Ho Ming and Yusof, Fadhilah},
  year = {2012},
  doi = {10.5120/7345-0160},
  journal = {International Journal of Computer Applications},
  number = {5},
  pages = {23--28},
  volume = {48}
}

@article{kimEtAl:2005,
  title = {Missing value estimation for {DNA} microarray gene expression data: local least squares imputation},
  author = {Kim, Hyunsoo and Golub, Gene H. and Park, Haesun},
  year = {2005},
  annote = {
The auhtors developed the Local Least Square imputation (LLSImputation) method by which similar 
k-neighbour genes are selected, then used to estimate a prediction model which is finally 
employed to predict the missing values. \\

The article compares LSSImputation with the KNNImputation and SVDimputation proposed by Troyanskaya 
\text{et al} (2001), and the Bayesian PCA proposed by Oba \textit{et al.} (2003). As the 
BPCA approach improves on the SVDImputation by incorporating Bayesian optimisation in a 
PC based method, LLSI improves on KNNI by combining the local similarity structure and 
the optimisation procedure of least squares. \\

Finally, the article directly confronts the issue of choosing the optimal number of k-nearest 
neighbours. In absence of clear theory, the authors propose to empirically identify on 
a case-by-case basis the value of \textit{k} by repeatedly predicting some artificially 
imposed missing values and selecting the one that best recover the known fabricated 
missing values.
},
  doi = {10.1093/bioinformatics/bth499},
  journal = {Bioinformatics},
  number = {2},
  pages = {187--198},
  volume = {21}
}

@article{kimEtAl:2004,
  title = {Reuse of imputed data in microarray analysis increases imputation efficiency},
  author = {Kim, Ki-Yeol and Kim, Byoung-Jin and Yi, Gwan-Su},
  year = {2004},
  doi = {10.1186/1471-2105-5-160},
  journal = {BMC bioinformatics},
  number = {160},
  volume = {5}
}

@article{langWu:2017,
  title = {A Comparison of Methods for Creating Multiple Imputations of Nominal Variables},
  author = {Kyle M. Lang and Wei Wu},
  year = {2017},
  doi = {10.1080/00273171.2017.1289360},
  journal = {Multivariate Behavioral Research},
  number = {3},
  pages = {290-304},
  publisher = {Routledge},
  volume = {52}
}

@article{lakshminarayanEtAl:1999,
  title = {Imputation of Missing Data in Industrial Databases},
  author = {Lakshminarayan, Kamakshi and Harp, Steven A. and Samad, Tariq},
  year = {1999},
  doi = {10.1023/A:1008334909089},
  journal = {Applied Intelligence},
  number = {3},
  pages = {259--275},
  volume = {11}
}

@article{langLittle:2018,
  title = {Principled missing data treatments},
  author = {Lang, Kyle M and Little, Todd D},
  year = {2018},
  journal = {Prevention Science},
  number = {3},
  pages = {284--294},
  publisher = {Springer},
  volume = {19}
}

@article{liaoEtAl:2014,
  title = {Missing value imputation in high-dimensional phenomic data: {Imputable} or not, and how?},
  author = {Liao, Serena G and Lin, Yan and Kang, Dongwan D and Chandra, Divay and Bon, Jessica and Kaminski, Naftali and Sciurba, Frank C and Tseng, George C},
  year = {2014},
  journal = {BMC bioinformatics},
  number = {1},
  pages = {346},
  publisher = {BioMed Central},
  volume = {15}
}

@article{littleSchluchter:1985,
  title = {Maximum likelihood estimation for mixed continuous and categorical variables with missing values},
  author = {Little, R. J. A. and Schluchter, M. D.},
  year = {1985},
  journal = {Biometrika},
  pages = {497--512},
  volume = {72}
}

@article{little:1988,
  title = {Missing-data adjustments in large surveys},
  author = {Little, Roderick J. A.},
  year = {1988},
  journal = {Journal of Business \& Economic Statistics},
  number = {3},
  pages = {287--296},
  volume = {6}
}

@article{littleEtAl:2013,
  title = {On the Joys of Missing Data},
  author = {Little, Todd D. and Jorgensen, Terrence D. and Lang, Kyle M. and Moore, E. W. G.},
  year = {2013},
  doi = {10.1093/jpepsy/jsto48},
  journal = {Journal of Pediatric Psychology},
  pages = {1--12}
}

@article{luengoEtAl:2010,
  title = {A study on the use of imputation methods for experimentation with Radial Basis Function Network classifiers handling missing attribute values: {The} good synergy between {RBFNs} and {EventCovering} method},
  author = {Luengo, Juli\'{a}n and Garc\'{i}a, Salvador and Herrara, Francisco},
  year = {2010},
  doi = {10.1016/j.neunet.2009.11.014},
  journal = {Neural Networks},
  number = {3},
  pages = {406--418},
  volume = {23}
}

@article{morganSonquist:1963,
  title = {Problems in the analysis of survey data, and a proposal},
  author = {Morgan, James N. and Sonquist, John A.},
  year = {1963},
  journal = {Journal of the American Statistical Association},
  number = {302},
  pages = {415--434},
  volume = {58}
}

@article{nanniEtAl:2012,
  title = {A classifier ensemble approach for the missing feature problem},
  author = {Nanni, Loris and Lumini, Alessandra and Brahnam, Sheryl},
  year = {2012},
  doi = {10.1016/j.artmed.2011.11.006},
  journal = {Artificial Intelligence in Medicine},
  number = {1},
  pages = {37--50},
  volume = {55}
}

@article{nordbotten:1995,
  title = {Editing Statistical Records by Neural Networks},
  author = {Nordbotten, Svein},
  year = {1995},
  journal = {Journal of Official Statistics},
  number = {4},
  pages = {391--411},
  url = {http://hdl.handle.net/11250/181393},
  volume = {11}
}

@article{nordbotten:1996,
  title = {Neural Network Imputation Applied to the Norwegian 1990 Population Census Data},
  author = {Nordbotten, Svein},
  year = {1996},
  journal = {Journal of Official Statistics},
  number = {4},
  pages = {385--401},
  url = {http://hdl.handle.net/11250/178156},
  volume = {12}
}

@article{oba2003bayesian,
  title = {A Bayesian missing value estimation method for gene expression profile data},
  author = {Oba, Shigeyuki and Sato, Masa-aki and Takemasa, Ichiro and Monden, Morito and Matsubara, Ken-ichi and Ishii, Shin},
  year = {2003},
  annote = {
The authors implemented a Bayesian Principle Component Regression apporach that 
performs better than KNNImpute and SVDImpute proposed by Troyanskaya2001.
},
  journal = {Bioinformatics},
  number = {16},
  pages = {2088--2096},
  publisher = {Oxford University Press},
  volume = {19}
}

@inproceedings{orchardWoodbury:1972,
  title = {A missing information principle: {Theory} and applications},
  author = {Orchard, Terence and Woodbury, Max A},
  year = {1972},
  annote = {
The authors provide a technical description of the \textit{Missing Information
Principle}, as defined through the decomposition of the information 
matrix for a vector of parameters of interest $\theta$.
The lost (missing) information is defined as the difference between the 
information matrix for $\theta$ in the hypothetical complete dataset,
and the information matrix for $\theta$ obtained with just the
observed cases. This decomposition is also used to describe the increase
in variance in the parameter estimates caused by the missing data. \\

For a more approachable definition of the Missing Information Principle see
Savalei and Rhemtulla (2012).
},
  booktitle = {Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Theory of Statistics},
  organization = {The Regents of the University of California}
}

@article{piela:2002,
  title = {Introduction to Self-Organizing Maps Modelling for Imputation---Techniques \& Technology},
  author = {Piela, P.},
  year = {2002},
  journal = {Research in Official Statistics},
  pages = {5--19},
  volume = {2}
}

@article{polikarEtAl:2010,
  title = {{Learn++.MF: A} random subspace approach for the missing feature problem},
  author = {Polikar, Robi and {DePasquale}, Joseph and Mohammed, Hussein Syed and Brown, Gavin and Kuncheva, Ludmilla I.},
  year = {2010},
  doi = {10.1016/j.patcog.2010.05.028},
  journal = {Pattern Recognition},
  number = {11},
  pages = {3817--3832},
  volume = {43}
}

@article{raghunathanEtAl:2001,
  title = {A multivariate technique for multiply imputing missing values using a sequence of regression models},
  author = {Raghunathan, Trivellore E. and Lepkowski, James M. and Van Hoewyk, John and Solenberger, Peter},
  year = {2001},
  annote = {
Raghunathan \textit{et al} introduce the SRMI (Sequential Regression Multivariate 
Imputation) approach to missing data imputation. The approach imputes the missing values 
on a variable-by-variable basis, by using posterior predictive distributions of the missing 
data, conditional on the observed data. \\

Apart from describing the principles, strengths, and weakness of the approach, their work also provides 
detailed instructions on how to perform multiple imputation according to SRMI (see Appendix A for 
a precise discussion on how to draw from a variety of regression models supported by SRMI). \\

Two case-studies and one simulation study are contextually presented and they are instrumental in 
showing how the SMRI approach performs compared to complete-case analysis and the Multiple 
Imputation based on a joint multivariate model.
},
  journal = {Survey Methodology},
  number = {1},
  pages = {85--96},
  volume = {27}
}

@article{reiter:2005,
  title = {Using {CART} to Generate Partially Synthetic, Public Use Microdata},
  author = {Reiter, J. P.},
  year = {2005},
  annote = {
The author proposes the use of CART to generate synthetic data sets (i.e. substituting sensitive observed
data points with multiply imputed values to avoid disclosure of information). For each of many
subsamples of the original dataset, the algorithm sequentially grows a tree for each of the k-th 
sensitive variables using all the other variables ($X, Y_{-k}$) as inputs, and then imputes the 
values to be replaces (the ``missing data points'') though bootstrap sampling of the $Y_k$ values
in the leaf where each observation falls. \\

The literature on using CART and other synthetic data generator methods is closely related to that of missing
data imputation. However, a few remarks are due. The first body of literature is concerned with
balancing between guarantying statistical validity of the secondary analyses and reducing the risk of
sensitive information disclosure, while missing data-handling literature is exclusively concerned
with the statistical validity issue. This is important because the different goal influences the tree 
pruning strategy. Furthermore, the concept of missing data mechanism is not relevant
for the synthetic data literature and replacement (missingness) is forced on either the entire variable
or just specific ranges of it (e.g. replace (impute) values of income which are greater then a 
threshold). The counterpart scenario in a missing data context is imputing a fully missing variable with
especially good initial guesses.
},
  journal = {Journal of Official Statistics},
  number = {3},
  pages = {7--30},
  volume = {21}
}

@article{rubin:1976,
  title = {Inference and missing data},
  author = {Rubin, Donald B},
  year = {1976},
  journal = {Biometrika},
  number = {3},
  pages = {581--592},
  publisher = {Oxford University Press},
  volume = {63}
}

@inproceedings{rubin:1978,
  title = {Multiple imputations in sample surveys -- {A} phenomenological {Bayesian} approach to nonresponse},
  author = {Rubin, Donald B},
  year = {1978},
  booktitle = {Proceedings of the survey research methods section of the {American Statistical Association}},
  organization = {American Statistical Association},
  pages = {20--34},
  volume = {1}
}

@article{rubin:1996,
  title = {Multiple imputation after 18+ years},
  author = {Rubin, Donald B},
  year = {1996},
  annote = {
In this work, Rubin reviews the state of multiple imputation 18 years after the publication 
of his seminal works in 1976 and 1978. This paper clarifies the two
main goals that Multiple Imputation was designed to achieve: the basic objective 
of allowing the data ultimate users to apply the same analytical methods as if 
the data had been complete; and the supplemental objective of granting analyses that 
are statistically valid for a scientific estimand. \\

The concept of statistical validity is operationalised with clarity by distinguishing between: 
randomisation validity and confidence validity. \\

Furthermore, the concept of \textit{proper} multiple imputation is discussed by 
summarising its main requirements. \\

Finally some criticisms to MI are addressed through the lenses provided by these concepts.
},
  journal = {Journal of the American Statistical Association},
  number = {434},
  pages = {473--489},
  volume = {91}
}

@article{rubin:1986,
  title = {Statistical matching using file concatenation with adjusted weights and multiple imputations},
  author = {Rubin, Donald B.},
  year = {1986},
  journal = {Journal of Business \& Economic Statistics},
  number = {1},
  pages = {87--94},
  volume = {4}
}

@article{saar-tsechanskyProvost:2007,
  title = {Handling Missing Values when Applying Classification Models},
  author = {Saar-Tsechansky, Maytal and Provost, Foster},
  year = {2007},
  journal = {Journal of Machine Learning Research},
  pages = {1217--1250},
  volume = {8}
}

@article{samadHarp:1992,
  title = {Self-organization with partial data},
  author = {Samad, Tariq and Harp, Steven A},
  year = {1992},
  doi = {10.1088/0954-898X/3/2/008},
  journal = {Network: Computation in Neural Systems},
  number = {2},
  pages = {205--212},
  volume = {3}
}

@article{savaleiRhemtulla:2012,
  title = {On obtaining estimates of the fraction of missing information from full information maximum likelihood},
  author = {Savalei, Victoria and Rhemtulla, Mijke},
  year = {2012},
  annote = {
This article provides a short yet insightful review of the Missing Information 
Principle (an interested reader may want to consult Orchard and Woodbury (1972)
for a more technical presentation of the same concept). According to this principle, 
the missing information for a parameter estimation procedure due to missing data 
is equal to the difference between the complete-data information matrix and the 
observed-data information matrix. \\

The definition of the Fraction of Missing Information (FMI) is explored
under both the Maximum Likelihood and the Multiple Imputation framework. The 
fundamental contribution of this paper is in fact showing how FMI is not a prerogative
of MI. \\

A final contribution is the detailed discussion of three different possible interpretations
of FMI: (relative) loss of (estimation) efficacy, loss of statistical power;
width inflation factor (i.e. how much bigger are the confidence intervals).     
},
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  number = {3},
  pages = {477--494},
  publisher = {Taylor \& Francis},
  volume = {19}
}

@article{sharpeSolly:1995,
  title = {Dealing with Missing Values in Neural Network-Based Diagnostic Systems},
  author = {Sharpe, P. K. and Solly, R. J.},
  year = {1995},
  doi = {10.1007/BF01421959},
  journal = {Neural Computing \& Applications},
  number = {2},
  pages = {73--77},
  volume = {3}
}

@article{silva-ramirezEtAl:2011,
  title = {Missing value imputation on missing completely at random data using multilayer perceptrons},
  author = {Silva-Ram\'{i}rez, Esther-Lydia and Pino-Mej\'{i}as, Rafael and L\'{o}pez-Coello, Manuel and Cubiles-de-la-Vega, Mar\'{i}a-Dolores},
  year = {2011},
  doi = {10.1016/j.neunet.2010.09.008},
  journal = {Neural Networks},
  number = {1},
  pages = {121--129},
  volume = {24}
}

@article{songBelin:2004,
  title = {Imputation for incomplete high-dimensional multivariate normal data using a common factor model},
  author = {Song, Juwon and Belin, Thomas R},
  year = {2004},
  doi = {10.1002/sim.1867},
  journal = {Statistics in Medicine},
  number = {18},
  pages = {2827--2843},
  publisher = {Wiley Online Library},
  volume = {23}
}

@article{songEtAl:2008,
  title = {Can \emph{k}-{NN} imputation improve the performance of {C4.5} with small software project data sets? {A} comparative evaluation},
  author = {Song, Qinbao and Shepperd, Martin and Chen, Xiangru and Liu, Jun},
  year = {2008},
  doi = {10.1016/j.jss.2008.05.008},
  journal = {The Journal of Systems and Software},
  number = {12},
  pages = {2361--2370},
  volume = {81}
}

@article{sterneEtAl:2009,
  title = {Multiple imputation for missing data in epidemiological and clinical research: {Potential} and pitfalls},
  author = {Sterne, Jonathan AC and White, Ian R and Carlin, John B and Spratt, Michael and Royston, Patrick and Kenward, Michael G and Wood, Angela M and Carpenter, James R},
  year = {2009},
  journal = {BMJ},
  pages = {b2393},
  publisher = {British Medical Journal Publishing Group},
  volume = {338}
}

@article{tannerWong:1987,
  title = {The calculation of posterior distributions by data augmentation},
  author = {Tanner, Martin A. and Wong, Wing Hung},
  year = {1987},
  journal = {Journal of the American Statistical Association},
  number = {398},
  pages = {528--540},
  volume = {82}
}

@article{troyanskayaEtAl:2001,
  title = {Missing value estimation methods for {DNA} microarrays},
  author = {Troyanskaya, Olga and Cantor, Michael and Sherlock, Gavin and Brown, Pat and Hastie, Trevor and Tibshirani, Robert and Botstein, David and Altman, Russ B.},
  year = {2001},
  annote = {
In the context of DNA microarrey studies with missing data, the authors show the better performance of a 
k-nearest neighbour imputation method (KNNImpute), over an SVD-based regression imputation method
(SVDImpute), and mean and zero imputation. \\

The KNNImpute algorithm outperforms all the other methods granting (1) less deterioration in performance
with increasing percentage of missingness, (2) robustness to the type of data considered (time-series 
or not, noisy or not), (3) less sensitivity to the number of parameters used (the choice of \textit{k}, 
the number of nearest neighbours considered for KNNImpute and the number of most significant eigengenes
selected for SVDImpute). \\

The generalisability of these results is somewhat hindered by two methodological choices:
\begin{itemize}
\item the missing data mechanisms considered is MCAR;
\item the metric used to identify the better method is the Root Mean Squared error, 
a standardised difference between the true data points values and the imputed ones.
As many contributions have highlighted (see Rubin, 1996; de Andrade Silva \textit{et al.}, 2013), the 
goal of missing data handling procedures is not necessarly recovering the true missing 
values but rather granting the statistical validity of the analyses performed on a 
dataset afflicted by missing data.
\end{itemize}
},
  doi = {10.1093/bioinformatics/17.6.520},
  journal = {Bioinformatics},
  number = {6},
  pages = {520--525},
  volume = {17}
}

@article{vanBuurenEtAl:2006,
  title = {Fully conditional specification in multivarite imputation},
  author = {van Buuren, Stef and Brand, J. P. L. and Groothuis-Oudshoorn, C. G. M. and Rubin, D. B.},
  year = {2006},
  annote = {
Different algorithms to perform Multiple Imputation by FCS, according to the
type of dependent variables, are presented in Appendix A of this paper.
Appendix B describes a method to generate nonmonotone multivariate
missing data under MAR. \\

In the main text, after a concise introduction to imputation by FCS, a brief but rigorous 
definition of \textit{compatibility} between conditional distributions is given. 
A good technical complement for an interested read is Arnold (2001). 
Simulations and study cases are then discussed 
in detail for both univariate and multivariate missing data scenarios, accounting for different
variable types (i.e. continuous, dichotomous, and polytomous). These studies
highlight the performances of FCS multiple imputation as compared to 
complete-case analysis in terms of bias and coverage of the confidence intervals. \\

Finally, a simulation study is performed to assess the consequences of (in)compatibility.
This is just an exemplifying scenario: not all possible incompatibility schemes can
be considered. Yet, it does provide compelling evidence in favour of FCS being 
robust to (clear) incompatibility.
},
  journal = {Journal of Statistical Computation and Simulation},
  number = {12},
  pages = {1049--1064},
  volume = {76}
}

@article{vonHippel:2007,
  title = {Regression with missing {Ys}: An improved strategy for analyzing multiply imputed data},
  author = {Von Hippel, Paul T.},
  year = {2007},
  journal = {Sociological Methodology},
  number = {1},
  pages = {83--117},
  volume = {37}
}

@article{vonHippel:2009,
  title = {How to impute interactions, squares, and other transformed variables},
  author = {Von Hippel, Paul T.},
  year = {2009},
  annote = {
Transformed variables such as squared and interaction terms need special attention
when Multiple Imputation is performed. This article compares two main methods
to impute them: the \textit{transform then impute} and the \textit{impute then transform} 
approach. \\

Through extreme missing data scenarios (100\% missignes), and example data analyses, the author
shows that the \textit{transform then impute} method is the one that best preserves
the mean and covariance structure of the original data and provides unbiased point
estimates of regression estimates. As a result, he strongly recommends such method. \\

Some variants of these methods are included in the comparison, namely \textit{passive imputation},
a more sophisticated but equally flawed version of the \textit{impute then transform} approach, 
and \textit{stratify, then impute} for interactions between categorical and a continuous
variables. \\

The main setup considered in this study is that of a linear regression model fitted to a
dataset with MAR missing data. However, the author addresses the extension of
the claims to models for binary dependent variables.    
},
  journal = {Sociological Methodology},
  number = {1},
  pages = {265--291},
  volume = {39}
}

@article{wallaceEtAl:2010,
  title = {A stochastic multiple imputation algorithm for missing covariate data in tree-structured survival analysis},
  author = {Wallace, Meredith L. and Anderson, Stewart J. and Mazumdar, Sati},
  year = {2010},
  annote = {
The authors propose a multiple imputation decision tree method that includes uncertainty regarding
the imputed values. This is achieved by slightly modifying the single imputation tree-based algorithm 
proposed by Conversano and Siciliano (2003): the tree is grown M times, and, after the first 
iteration, the imputed variables are considered as possible candidates for the split as well 
as the complete ones; once the best splitting variable and values are chosen, the algorithm 
classifies the cases with missing values on the target variable, based on their observed 
values on the other features, in a given node, and imputes that node mean value of the target feature. 
Each time a value is imputed with the node's mean, a random error is added to it. As a result 
each missing data point is filled with M different values, allowing the algorithm account for 
uncertainty of the imputed value.
},
  doi = {10.1002/sim.4079},
  journal = {Statistics in Medicine},
  number = {29},
  pages = {3004--3016},
  volume = {29}
}

@article{wangEtAl:2006,
  title = {Missing value estimation for DNA microarray gene expression data by Support Vector Regression imputation and orthogonal coding scheme},
  author = {Wang, Xian and Li, Ao and Jiang, Zhaohui and Feng, Huanqing},
  year = {2006},
  doi = {10.1186/1471-2105-7-32},
  journal = {{BMC} Bioinformatics},
  number = {32},
  pages = {1--10},
  volume = {7}
}

@article{wasitoMirkin:2005,
  title = {Nearest neighbour approach in the least-squares data imputation algorithms},
  author = {Wasito, Ito and Mirkin, Boris},
  year = {2005},
  annote = {
The authors introduce the INImpute approach, an algorithm that combines an iterative SVD-based 
least-square imputation with a nearest neighbour approach. Missing values are imputed 
first globally (i.e. considering the entire dataset) through an Iterative Majorization Least Square 
algorithm (an SVD/PCR-based imputation method with 4 principal components, p=4). Then, a kNN algorithm 
is used to select k-nearest-neighbours to an instance that had a missing value for a particular variable, 
and replaces the previously (globally) imputed value with a new one found with anoter IMLS run only among the
nearest-neighbours (and with p=1). \\

The theoretical properties of this approach are not discussed but the authors do show the superiority
of INImpute to regular kNN and other ILS approaches in a variety of scenarios.
},
  doi = {10.1016/j.ins.2004.02.014},
  journal = {Information Sciences},
  number = {1--2},
  pages = {1--25},
  volume = {169}
}

@article{wasitoMirkin:2006,
  title = {Nearest neighbour approach in the least-squares data imputation algorithms with different missing patterns},
  author = {Wasito, Ito and Mirkin, Boris},
  year = {2006},
  annote = {
The authors extend the comparison of the different least-squares imputation techniques performed by Wasito 
and Mirkin (2005) to accommodate for different missing data mechanisms (MCAR, MNAR and a merged data 
missingness). \\

The results mainly show that all versions of Nearest-Neighbour-based least-squares imputations presented
outperform the global versions. Furthermore, the global-local INI (IMLS-NN-IMLS) approach wins 
in almost all contexts except when only the local version (the NN-IMLS) wins.

The article also includes a detailed description of a data generating process according to the Neural Network 
NetLab framework, and of the missing data patterns generation. Hence, it is a good reference for anyone
planning a simulation study to test imputation methods.
},
  doi = {10.1016/j.csda.2004.11.009},
  journal = {Computational Statistics \& Data Analysis},
  number = {4},
  pages = {926--949},
  volume = {50}
}

@article{whiteEtAl:2011,
  title = {Multiple imputation using chained equations: {Issues} and guidance for practice},
  author = {White, Ian R and Royston, Patrick and Wood, Angela M},
  year = {2011},
  annote = {

White's contribution provides comparative guidelines and recommendations 
on different aspects of MI:

\begin{itemize}

\item Handling different dependent variable distributions. In particular, there is
a helpful section dealing with how to handle imputation for skewed continuous 
variables.

\item The question of which variables to include is addressed along with the issue 
of preserving all the relationships included in the analysis model, when 
defining the imputation model. Three main ways of dealing with this issues 
are presented (i.e. passive approach, improved passive approach using PMM, 
and JAV).

\item Number of imputations ‚Äì The authors criticise the \textit{efficiency argument} (that
usually leads to the rule of thumb  \textit{m} = 5 is adequate for FMI $\leq$ .25) through
the \textit{replicability argument}, centring the discussion around the decision of \textit{m}
in terms of Monte Carlo errors. The author's rule of thumb is that \textit{m} $\geq$ \% of 
incomplete cases.

\item Limitations and pitfalls of MI ‚Äì Apart from the lack of theoretical background,
the authors discuss the following pitfalls: perfect prediction (potentially a problem
when the dependent variable is categorical); sensitivity to MAR violation, non-convergence 
issues; and the problem of too many variables.

\end{itemize}

},
  journal = {Statistics in Medicine},
  number = {4},
  pages = {377--399},
  publisher = {Wiley Online Library},
  volume = {30}
}

@article{yoonLee:1999,
  title = {Training Algorithm with Incomplete Data for Feed-Forward Neural Networks},
  author = {Yoon, Song-Yee and Lee, Soo-Young},
  year = {1999},
  doi = {10.1023/A:1018772122605},
  journal = {Neural Processing Letters},
  number = {3},
  pages = {171--179},
  volume = {10}
}

@article{zhang:2012,
  title = {Nearest neighbor selection for iteratively \emph{k}{NN} imputation},
  author = {Zhang, Shichao},
  year = {2012},
  annote = {
The author introduces the Gray k-Nearest-Neighbour (GkNN) algorithm as an improvement to the traditional 
kNNImpute algorithm (see Troyanskaya \textit{et al.} 2001). This new version uses a Gray Relation
Grade measure of similarity, from Gray Relational Analysis, which easily 
accommodates for both numerical and categorical input variables. \\

Furthermore, GkNN is an iterative algorithm rooted in the EM framework, and the author claims that such feature makes the
algorithm able to account for the uncertainty related to the imputation procedure. However, it is not explicit how
this algorithm takes into account the additional uncertainty regarding parameters estimates. \\

Finally, the GkNN algorithm
uses all the information included in the dataset by including in the imputation of an instance 
\text{i}, the observed and imputed values of other instances that have missing values.
},
  doi = {10.1016/j.jss.2012.05.073},
  journal = {The Journal of Systems and Software},
  number = {11},
  pages = {2541--2552},
  volume = {85}
}

@article{zhaoLong:2016,
  title = {Multiple imputation in the presence of high-dimensional data},
  author = {Zhao, Yize and Long, Qi},
  year = {2016},
  annote = {
The authors discuss the used of chained equations regularised regression imputation that enables missing data handling 
within the Multiple Imputation framework with high-dimensional data ($p > n$). Three main approaches are proposed:
a direct use of regularised regression on multiple bootstrapped datasets (DURR), an inderect use of it, and finally
a bayesian lasso approach. \\

The method BLasso method seems to outperform the other ones. Furthermore, it can easily be extended to general missing 
data patters (the study only shows results for univariate missing data), while the other methods do not share such
feature. However, the BLasso approach becomes computationally extremely intensive (even infeasible) when many 
variables are afflicted by missing values.
},
  doi = {10.1177/0962280213511027},
  journal = {Statistical Methods in Medical Research},
  number = {5},
  pages = {2021--2035},
  publisher = {SAGE Publications Sage UK: London, England},
  volume = {25}
}

@article{zhuKosorok:2012,
  title = {Recursively Imputed Survival Trees},
  author = {Zhu, Ruoqing and Kosorok, Michael R.},
  year = {2012},
  doi = {10.1080/01621459.2011.637468},
  journal = {Journal of the American Statistical Association},
  number = {497},
  pages = {331--340},
  volume = {107}
}

@article{rey-del-castilloCardenosa:2012,
  title = {Fuzzy min--max neural networks for categorical data: application to missing data imputation},
  author = {{Rey-del-Castillo}, Pilar and Carde\~{n}osa, Jes\'{u}s},
  year = {2012},
  doi = {10.1007/s00521-011-0574-x},
  journal = {Neural Computing \& Applications},
  number = {6},
  pages = {1349--1362},
  volume = {21}
}

